# Configuration

# ==================================== Feishu(Lark) Configuration ==================================== #
# ------------------------------------------------------------------------------------------------------------ #
# Feishu(Lark) Bot Webhook URL
webhook_url: ''  # TODO: Change to your Webhook URL

# Feishu(Lark) Card Template
template_id: 'AAqhkIh9vH2vn'  # TODO: Change to your template_id
template_version_name: '1.0.1'  # TODO: Change to your template_version_name
# ------------------------------------------------------------------------------------------------------------ #


# ==================================== Paper Configuration ==================================== #
# ------------------------------------------------------------------------------------------------------------ #
tag: 'LLM Agent'  # Tag for Feishu(Lark) Card

category_list:  # arXiv categories to search for papers
  - cs.CL  # Computation and Language
  - cs.AI  # Artificial Intelligence
  - cs.CV  # Computer Vision and Pattern Recognition
  # - cs.CR  # Cryptography and Security
  # - cs.LG  # Machine Learning

keyword_list:  # Keywords to filter papers
  - agent
  - RL
  - Reinforcement learning
  - agentic
  - -Embodied
  - -Histopathology
  - -Vehicle
  - -UAV
  - -Protein
  - -Traffic
  - -biomedical
  - -clinical
  - -image generation
  - -Video
  - -Music
  - -Autonomous Driving
# ------------------------------------------------------------------------------------------------------------ #


# ==================================== LLM Service Configuration ==================================== #
# ------------------------------------------------------------------------------------------------------------ #

# LLM Server Config
model: 'gpt-4o-mini-2024-07-18'
base_url: ''  # NOTE: For ollama, need to add '/v1' at the end of the OLLAMA_HOST URL
api_key: ''

#### >>> LLM Server Config EXAMPLE >>> ####

## 1. For Ollama Server ##
# model: 'qwen2.5:7b'
# base_url: 'http://localhost:11434/v1'  # NOTE: For ollama, need to add '/v1' at the end of the OLLAMA_HOST URL
# api_key: 'ollama'  # Any non-empty string works (Ollama does not require authentication)

## 2. For Other OpenAI SDK-Compatibale LLM Server ##
# model: 'gpt-4o-mini'
# base_url: 'https://api.openai.com/v1'
# api_key: 'sk-xxxxx'

#### <<< LLM Server Config EXAMPLE <<< ####

# ------------------------------------------------------------------------------------------------------------ #

# Use LLM for More Accurate Paper Filtering
use_llm_for_filtering: false  # Set to false to disable LLM-based filtering

#### >>> LLM-Based Paper Filtering >>> ####
# If set to true, `paper_to_hunt.md` file in the project root directory will be used for LLM-based filtering.
# You can modify the prompt in `paper_to_hunt.md` to describe the paper you want to hunt for.
#
# If you want to use LLM-Based Filtering **only** (without Keyword Filtering), set `keyword_list` to an empty list like below:
# keyword_list: []
#### <<< LLM-Based Paper Filtering <<< ####

# ------------------------------------------------------------------------------------------------------------ #

# Use LLM for Paper Abstract Translation
use_llm_for_translation: true  # Set to false to disable LLM-based translation

# ------------------------------------------------------------------------------------------------------------ #
